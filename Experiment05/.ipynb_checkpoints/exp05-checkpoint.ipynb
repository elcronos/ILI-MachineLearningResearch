{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import numpy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import RegexpStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_json(\"../../data/RelatedVsNotRelated.json\")\n",
    "train2 = pd.read_json(\"../../data/AwarenessVsInfection.json\")\n",
    "train3 = pd.read_json(\"../../data/SelfVsOthers.json\")\n",
    "\n",
    "# Load Most Common Words\n",
    "most_common  = pd.read_csv(\"./most_common/related.csv\")\n",
    "most_common2 = pd.read_csv(\"./most_common/infection.csv\")\n",
    "most_common3 = pd.read_csv(\"./most_common/self.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "related = word_tokenize(' '.join(most_common['Word']))\n",
    "infection = word_tokenize(' '.join(most_common2['Word']))\n",
    "self = word_tokenize(' '.join(most_common3['Word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Word Stops\n",
    "stopset = set(stopwords.words('english'))\n",
    "morewords = ['who','which', 'I\\'m','\\'m']\n",
    "stopset.update(morewords)\n",
    "\n",
    "# Remove URLs, RTs, and twitter handles\n",
    "def clean_data(text):\n",
    "    # Remove unicode characters\n",
    "    text = text.replace('[^\\x00-\\x7F]','')\n",
    "    words = [text for text in text.split() if 'http' not in text and not text.startswith('@') and text != 'RT']\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Text to Lower Case\n",
    "def text_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove some characters\n",
    "def remove_special_characters(text):\n",
    "    bad_chars = '#?(){}<>:;.!$%&/=+*^-'\n",
    "    rgx = re.compile('[%s]' % bad_chars)\n",
    "    return rgx.sub('', text)\n",
    "\n",
    "# Create a set of Stopwords\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopset]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopset:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Stemming words\n",
    "def stem_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    #Regex for Suffixes\n",
    "    st = RegexpStemmer('ing$|s$|e$|able$|ible$|ful$|less$|ive$|acy$|al$|ance$|ence$|dom$|er$|or$|ism$|ist$|ity$|ty$|ment$|ship$|sion$|tion$|ate$|en$|ify$|fy$|ize$|ise$', min=4)\n",
    "    stemmed = []\n",
    "    for word in words:\n",
    "        stemmed.append(st.stem(word))\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    for i, row in df.iterrows():\n",
    "      cleaned_text = row['text']\n",
    "      cleaned_text= clean_data(cleaned_text)\n",
    "      cleaned_text= text_to_lower(cleaned_text)\n",
    "      cleaned_text= remove_special_characters(cleaned_text)\n",
    "      cleaned_text= remove_stopwords(cleaned_text)\n",
    "      cleaned_text= stem_words(cleaned_text)\n",
    "      df.set_value(i,'text',cleaned_text)\n",
    "    return df\n",
    "\n",
    "def create_cvs(text, name_file, number):\n",
    "\n",
    "    with open('./most_common/'+name_file+'.csv', 'w') as fp:\n",
    "        a = csv.writer(fp, delimiter=',')\n",
    "        data = [['Word', 'Frequency']]\n",
    "        # Calculate frequency distribution\n",
    "        fdist = nltk.FreqDist(text)\n",
    "        for word, frequency in fdist.most_common(number):\n",
    "            data.append([word, frequency])\n",
    "        a.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flu',\n",
       " 'gett',\n",
       " 'shot',\n",
       " 'think',\n",
       " 'sick',\n",
       " 'get',\n",
       " 'worried',\n",
       " 'feel',\n",
       " 'go',\n",
       " 'h1n1',\n",
       " 'lik',\n",
       " 'got',\n",
       " 'scared',\n",
       " 'hop',\n",
       " 'bett',\n",
       " 'worry',\n",
       " 'fear',\n",
       " 'vaccin',\n",
       " 'today',\n",
       " 'one',\n",
       " 'still',\n",
       " 'need',\n",
       " 'cold',\n",
       " 'really',\n",
       " 'hom']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean text on my Dataframe\n",
    "train = clean_text(train)\n",
    "train2 = clean_text(train2)\n",
    "train3 = clean_text(train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flu',\n",
       " 'gett',\n",
       " 'shot',\n",
       " 'think',\n",
       " 'sick',\n",
       " 'feel',\n",
       " 'get',\n",
       " 'got',\n",
       " 'go',\n",
       " 'lik',\n",
       " 'im',\n",
       " 'bett',\n",
       " 'hop',\n",
       " 'worried',\n",
       " 'today',\n",
       " 'still',\n",
       " 'day',\n",
       " 'scared',\n",
       " 'week',\n",
       " 'vaccin',\n",
       " 'good',\n",
       " 'cold',\n",
       " 'worry',\n",
       " 'work',\n",
       " 'back']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_related = sklearn.feature_extraction.text.CountVectorizer(vocabulary=related)\n",
    "list_related = train['text'].tolist()\n",
    "\n",
    "cv_infection = sklearn.feature_extraction.text.CountVectorizer(vocabulary=infection)\n",
    "list_infection = train2['text'].tolist()\n",
    "\n",
    "cv_self = sklearn.feature_extraction.text.CountVectorizer(vocabulary=self)\n",
    "list_self = train3['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_related = cv_related.fit_transform(list_related).toarray()\n",
    "numpy.savetxt(\"related.csv\", numpy.asarray(array_related), fmt='%i', delimiter=\",\")\n",
    "\n",
    "array_infection = cv_infection.fit_transform(list_infection).toarray()\n",
    "numpy.savetxt(\"infection.csv\", numpy.asarray(array_infection), fmt='%i', delimiter=\",\")\n",
    "\n",
    "array_self = cv_self.fit_transform(list_self).toarray()\n",
    "numpy.savetxt(\"self.csv\", numpy.asarray(array_self), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo =  pd.read_csv(\"related.csv\")\n",
    "foo['RESULT'] = Series(train['type'], index=foo.index)\n",
    "foo['ID'] = Series(train['id'], index=foo.index)\n",
    "foo.to_csv('./data_vectorised/related.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo =  pd.read_csv(\"infection.csv\")\n",
    "foo['RESULT'] = Series(train['type'], index=foo.index)\n",
    "foo['ID'] = Series(train['id'], index=foo.index)\n",
    "foo.to_csv('./data_vectorised/infection.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo =  pd.read_csv(\"self.csv\")\n",
    "foo['RESULT'] = Series(train['type'], index=foo.index)\n",
    "foo['ID'] = Series(train['id'], index=foo.index)\n",
    "foo.to_csv('./data_vectorised/self.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
