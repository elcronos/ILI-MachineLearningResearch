{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flu,779\n",
      "swin,387\n",
      "gett,310\n",
      "'s,143\n",
      "worried,107\n",
      "n't,99\n",
      "think,74\n",
      "sick,73\n",
      "scared,70\n",
      "get,69\n",
      "shot,62\n",
      "worry,58\n",
      "got,56\n",
      "bird,56\n",
      "fear,56\n",
      "feel,53\n",
      "bett,49\n",
      "h1n1,48\n",
      "hop,45\n",
      "everyon,43\n",
      "lik,43\n",
      "go,41\n",
      "today,38\n",
      "peopl,38\n",
      "tak,38\n",
      "u,38\n",
      "afraid,36\n",
      "kid,34\n",
      "hom,33\n",
      "catch,32\n",
      "2,29\n",
      "know,27\n",
      "seem,26\n",
      "hand,26\n",
      "'',26\n",
      "``,26\n",
      "check,25\n",
      "week,25\n",
      "one,24\n",
      "'re,24\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "train = pd.read_json(\"../../data/RelatedVsNotRelated.json\")\n",
    "train2 = pd.read_json(\"../../data/AwarenessVsInfection.json\")\n",
    "train3 = pd.read_json(\"../../data/SelfVsOthers.json\")\n",
    "\n",
    "# ## Related Vs Not Related:\n",
    "#  0: Not related to influenza\n",
    "#  1: Related to influenza\n",
    "\n",
    "train_not_related = train.loc[train['type'] == 0]\n",
    "train_related = train.loc[train['type'] == 1]\n",
    "\n",
    "\n",
    "# ## Awareness Vs Infection\n",
    "\n",
    "# 0: Influenza infection\n",
    "# 1: Influenza awareness\n",
    "\n",
    "train_infection = train2.loc[train2['type'] == 0]\n",
    "train_awareness = train2.loc[train2['type'] == 1]\n",
    "\n",
    "\n",
    "# ## Self Vs Others\n",
    "\n",
    "# 0: Others (the tweet describes someone else)\n",
    "# 1: Self (the tweet describes the author)\n",
    "\n",
    "train_others = train3.loc[train3['type'] == 0]\n",
    "train_self = train3.loc[train3['type'] == 1]\n",
    "\n",
    "\n",
    "# ## Methods\n",
    "\n",
    "# Define Word Stops\n",
    "stopset = set(stopwords.words('english'))\n",
    "morewords = ['who','which','isn\\'t','aren\\'t', 'I\\'m','\\'m']\n",
    "stopset.update(morewords)\n",
    "\n",
    "# Remove URLs, RTs, and twitter handles\n",
    "def clean_data(text):\n",
    "    text = text.replace('[^\\x00-\\x7F]','')\n",
    "    words = [text for text in text.split() if 'http' not in text and not text.startswith('@') and text != 'RT']\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Text to Lower Case\n",
    "def text_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove some characters\n",
    "def remove_special_characters(text):\n",
    "    bad_chars = '#?(){}<>:;.!$%&/=+*^-'\n",
    "    rgx = re.compile('[%s]' % bad_chars)\n",
    "    return rgx.sub('', text)\n",
    "\n",
    "# Create a set of Stopwords\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopset]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopset:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Stemming words\n",
    "def stem_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    #Regex for Suffixes\n",
    "    st = RegexpStemmer('ing$|s$|e$|able$|ible$|ful$|less$|ive$|acy$|al$|ance$|ence$|dom$|er$|or$|ism$|ist$|ity$|ty$|ment$|ship$|sion$|tion$|ate$|en$|ify$|fy$|ize$|ise$', min=4)\n",
    "    stemmed = []\n",
    "    for word in words:\n",
    "        stemmed.append(st.stem(word))\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    for i, row in df.iterrows():\n",
    "      cleaned_text = row['text']\n",
    "      cleaned_text= clean_data(cleaned_text)\n",
    "      cleaned_text= text_to_lower(cleaned_text)\n",
    "      cleaned_text= remove_special_characters(cleaned_text)\n",
    "      cleaned_text= remove_stopwords(cleaned_text)\n",
    "      cleaned_text= stem_words(cleaned_text)\n",
    "      df.set_value(i,'text',cleaned_text)\n",
    "    return df\n",
    "\n",
    "def create_wordcloud(list_words, name_cloud):\n",
    "    wordcloud = WordCloud(\n",
    "                      stopwords= stopset,\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(list_words)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('./wordclouds/'+name_cloud, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def print_frequency(words, number):\n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    for word, frequency in fdist.most_common(number):\n",
    "        print('{},{}'.format(word, frequency))\n",
    "\n",
    "\n",
    "\n",
    "#Clean text on my Dataframe\n",
    "train_related = clean_text(train_related)\n",
    "train_not_related= clean_text(train_not_related)\n",
    "\n",
    "train_infection = clean_text(train_infection)\n",
    "train_awareness = clean_text(train_awareness)\n",
    "\n",
    "train_others = clean_text(train_others)\n",
    "train_self = clean_text(train_self)\n",
    "\n",
    "# Tokenizing DF\n",
    "list1 = nltk.tokenize.word_tokenize(' '.join(train_related['text']))\n",
    "list2 = nltk.tokenize.word_tokenize(' '.join(train_not_related['text']))\n",
    "list3 = nltk.tokenize.word_tokenize(' '.join(train_infection['text']))\n",
    "list4 = nltk.tokenize.word_tokenize(' '.join(train_awareness['text']))\n",
    "list5 = nltk.tokenize.word_tokenize(' '.join(train_others['text']))\n",
    "list6 = nltk.tokenize.word_tokenize(' '.join(train_self['text']))\n",
    "\n",
    "#print_frequency(list1, 50)\n",
    "print_frequency(list5,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
