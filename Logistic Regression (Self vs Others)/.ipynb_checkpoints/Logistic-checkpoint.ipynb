{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from scikitplot import plotters as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import RegexpStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train = pd.read_json(\"../../data/SelfVsOthers.json\")\n",
    "train_others = train.loc[train['type'] == 0]\n",
    "train_self = train.loc[train['type'] == 1]\n",
    "\n",
    "# Define Word Stops\n",
    "stopset = set(stopwords.words('english'))\n",
    "morewords = [\"'s\", \"swine\", \"bird\", \"h1n1\", \"'ve\", \"lol\", \"pig\"]\n",
    "stopset.update(morewords)\n",
    "#Remove word from stopword list\n",
    "itemsToRemove = ['can','am', 'are', 're', 'm','have','has','i', 'you', 'he', 'she', 'we', 'they']\n",
    "stopset = [x for x in stopset if x not in itemsToRemove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Methods\n",
    "# Remove URLs, RTs, and twitter handles\n",
    "def clean_data(text):\n",
    "    text = text.replace('[^\\x00-\\x7F]','')\n",
    "    words = [text for text in text.split() if 'http' not in text and not text.startswith('@') and text != 'RT']\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Text to Lower Case\n",
    "def text_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove some characters\n",
    "def remove_special_characters(text):\n",
    "    bad_chars = '-#?(){}<>:;.!$%&/=+*^-`\\'0123456789'\n",
    "    rgx = re.compile('[%s]' % bad_chars)\n",
    "    return rgx.sub('', text)\n",
    "\n",
    "# Create a set of Stopwords\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopset]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopset:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Stemming words\n",
    "def stem_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    #Regex for Suffixes\n",
    "    st = RegexpStemmer('ing$|s$|able$|ible$|ful$|less$|ive$|acy$|al$|ance$|ence$|dom$|er$|or$|ism$|ist$|ity$|ty$|ment$|ship$|sion$|tion$|ate$|en$|ify$|fy$|ize$|ise$', min=4)\n",
    "    stemmed = []\n",
    "    for word in words:\n",
    "        stemmed.append(st.stem(word))\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    for i, row in df.iterrows():\n",
    "      cleaned_text = row['text']\n",
    "      cleaned_text= clean_data(cleaned_text)\n",
    "      cleaned_text= text_to_lower(cleaned_text)\n",
    "      cleaned_text= remove_special_characters(cleaned_text)\n",
    "      cleaned_text= remove_stopwords(cleaned_text)\n",
    "      cleaned_text= stem_words(cleaned_text)\n",
    "      df.set_value(i,'text',cleaned_text)\n",
    "    return df\n",
    "\n",
    "def create_wordcloud(list_words, name_cloud):\n",
    "    wordcloud = WordCloud(\n",
    "                      stopwords= stopset,\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(list_words)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('./wordclouds/'+name_cloud, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def print_frequency(words, number):\n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    for word, frequency in fdist.most_common(number):\n",
    "        print('{},'.format(word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clean text on my Dataframe\n",
    "train_others = clean_text(train_others)\n",
    "train_self = clean_text(train_self)\n",
    "# Clean text on my Dataframe\n",
    "train = clean_text(train)\n",
    "# Tokenizing DF\n",
    "list_words = nltk.tokenize.word_tokenize(' '.join(train_self['text']))\n",
    "#print_frequency(list_words,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vectorisation\n",
    "most_common  = pd.read_csv(\"./predictors.csv\")\n",
    "self = word_tokenize(' '.join(most_common))\n",
    "\n",
    "cv_self = sklearn.feature_extraction.text.CountVectorizer(vocabulary=self)\n",
    "list_self = train['text'].tolist()\n",
    "\n",
    "array_self = cv_self.fit_transform(list_self).toarray()\n",
    "# Create CSV file\n",
    "#numpy.savetxt(\"self.csv\", np.asarray(array_self.astype(int)), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#foo =  pd.read_csv(\"self.csv\")\n",
    "#foo['RESULT'] = Series(train['type'], index=foo.index)\n",
    "#foo['ID'] = Series(train['id'], index=foo.index)\n",
    "#foo.to_csv('./data_vectorised/self.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72587222473946533"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data_vectorised/self.csv\")\n",
    "y, X = dmatrices(\"RESULT ~ flu + gett + i + im  + he + school + sleep + dr + catch + make + tomorrow + since + damn + bit + great + keep + h + tired + first + soon + everyone + away + head + thought + someth + ready + next + start + com + fuck + may + little + anyone + lot + body + doct + could + long + god + seem + night + man + care + ok + done + look + stay + weekend + say + eith + nose + isnt + tonight + tell + office + regular + hand + shit + enough + come + rest + mask + help + please + fun + stomach + would + re + yes + sure + stupid + viru + nervou + due + crap + tak + cause + l + hopefully + life + old + wond + yeah + hell + health + woke + disease + clinic + every + must + suck + many + someone + actually + least + headache + kind + nas + concerned + havent + nev + hurt + youre + love + girl + friend + read + hour + hard + b + also + bc + us + anyth + g + ch\", data, return_type = 'dataframe')\n",
    "# flatten y into a 1-D array\n",
    "y = np.ravel(y)\n",
    "\n",
    "# instantiate a logistic regression model, and fit with X and y\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X,y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Evaluation Using a Validation Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "#print predicted\n",
    "y_score = model.fit(X_train, y_train).decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate class probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "#print probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.704374057315\n",
      "0.732988537004\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "print metrics.accuracy_score(y_test, predicted)\n",
    "print metrics.roc_auc_score(y_test, probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 65 147]\n",
      " [ 49 402]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.31      0.40       212\n",
      "        1.0       0.73      0.89      0.80       451\n",
      "\n",
      "avg / total       0.68      0.70      0.67       663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print metrics.confusion_matrix(y_test, predicted)\n",
    "print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "probas = model.predict_proba(X)\n",
    "skplt.plot_roc_curve(y_true=y, y_probas=probas)\n",
    "#plt.show()\n",
    "# Confusion Matrix\n",
    "preds = model.predict(X)\n",
    "skplt.plot_confusion_matrix(y_true=y, y_pred=preds)\n",
    "#plt.show()\n",
    "#Learning Curve\n",
    "skplt.plot_learning_curve(model, X, y)\n",
    "#plt.show()\n",
    "\n",
    "#Precision Recall Curve\n",
    "skplt.plot_precision_recall_curve(y_true=y, y_probas=probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
