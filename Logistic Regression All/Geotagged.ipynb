{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-cbdbb18ba904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/modelLogistic.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#TweetsAU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mdata09\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/tweetDB-AU-Nov-17.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-cbdbb18ba904>\u001b[0m in \u001b[0;36mloadjson\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# now, load it into pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#sample = pd.read_json(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0;31m# TODO speed up Series case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_from_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from pandas import Series\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RegexpStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def readjson(path):\n",
    "    return pd.read_json(path)\n",
    "\n",
    "def loadjson(path):\n",
    "\n",
    "    # read the entire file into a python array\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.readlines()\n",
    "    # remove the trailing \"\\n\" from each line\n",
    "    data = map(lambda x: x.rstrip(), data)\n",
    "    #data = ''.join(data)\n",
    "    data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "    #print data_json_str\n",
    "    # now, load it into pandas\n",
    "    #sample = pd.read_json(data)\n",
    "    sample = pd.DataFrame.from_dict(data, orient='index').T.set_index('index')\n",
    "    return sample\n",
    "\n",
    "def loadjson2(path):\n",
    "    # Reading the json as a dict\n",
    "    with open(path) as json_data:\n",
    "        data = json.load(json_data)\n",
    "    # using the from_dict load function. Note that the 'orient' parameter\n",
    "    #is not using the default value (or it will give the same error than you had)\n",
    "    # We transpose the resulting df and set index column as its index to get this result\n",
    "    jsonfile = pd.DataFrame.from_dict(data, orient='index').T.set_index('index')\n",
    "    return jsonfile\n",
    "\n",
    "#Classifier Load\n",
    "clf = joblib.load('./model/modelLogistic.pkl')\n",
    "#TweetsAU\n",
    "data09 = loadjson(\"./data/tweetDB-AU-Nov-17.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(data09)\n",
    "columns=['PREDICTION','PROBABILITY','ID','IDUSER','LOCATION','LAT','LON','TIMEZONE']\n",
    "df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Word Stops\n",
    "stopset = set(stopwords.words('english'))\n",
    "morewords = [\"'s\", \"swine\", \"bird\", \"h1n1\", \"'ve\", \"lol\", \"pig\"]\n",
    "stopset.update(morewords)\n",
    "#Remove word from stopword list\n",
    "itemsToRemove = ['can','am', 'are', 're', 'm','have','has','i', 'you', 'he', 'she', 'we', 'they']\n",
    "stopset = [x for x in stopset if x not in itemsToRemove]\n",
    "\n",
    "#Vectorisation\n",
    "predictors  = pd.read_csv(\"./predictors.csv\")\n",
    "vocabulary = word_tokenize(' '.join(predictors))\n",
    "count_vector = CountVectorizer(vocabulary=vocabulary)\n",
    "#Predictors String\n",
    "predictor_list=list(predictors)\n",
    "predictors_str = ','.join(predictor_list)\n",
    "\n",
    "#Methods\n",
    "# Remove URLs, RTs, and twitter handles\n",
    "def clean_data(text):\n",
    "    #text= text.decode('utf-8')\n",
    "    text = text.replace('[^\\x00-\\x7F]','')\n",
    "    words = [text for text in text.split() if 'http' not in text and not text.startswith('@') and text != 'RT']\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Text to Lower Case\n",
    "def text_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove some characters\n",
    "def remove_special_characters(text):\n",
    "    bad_chars = '-#?(){}<>:;.!$%&/=+*^-`\\'0123456789'\n",
    "    rgx = re.compile('[%s]' % bad_chars)\n",
    "    return rgx.sub('', text)\n",
    "\n",
    "# Create a set of Stopwords\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopset]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopset:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Stemming words\n",
    "def stem_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    #Regex for Suffixes\n",
    "    st = RegexpStemmer('ing$|s$|able$|ible$|ful$|less$|ive$|acy$|al$|ance$|ence$|dom$|er$|or$|ism$|ist$|ity$|ty$|ment$|ship$|sion$|tion$|ate$|en$|ify$|fy$|ize$|ise$', min=4)\n",
    "    stemmed = []\n",
    "    for word in words:\n",
    "        stemmed.append(st.stem(word))\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def get_cleaned_text(text):\n",
    "    try:\n",
    "        cleaned_text= clean_data(text)\n",
    "        cleaned_text= text_to_lower(cleaned_text)\n",
    "        cleaned_text= remove_special_characters(cleaned_text)\n",
    "        cleaned_text= remove_stopwords(cleaned_text)\n",
    "        cleaned_text= stem_words(cleaned_text)\n",
    "    except Exception: \n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def clean_text(df):\n",
    "    for i, row in df.iterrows():\n",
    "        cleaned_text = row['text']\n",
    "        cleaned_text = get_cleaned_text(cleaned_text)\n",
    "        df.set_value(i,'text',cleaned_text)\n",
    "    return df\n",
    "\n",
    "def get_vector(text):\n",
    "    array_vector = count_vector.fit_transform([text]).toarray()[0]\n",
    "    return array_vector\n",
    "\n",
    "def classifier(X):\n",
    "    return clf.predict(X)\n",
    "\n",
    "def probability(X):\n",
    "    return clf.predict_proba(X)\n",
    "\n",
    "def text_classify(text):\n",
    "    text= get_cleaned_text(text)\n",
    "    X = [1] #Interceptor\n",
    "    X2 = count_vector.fit_transform([text]).toarray()[0]\n",
    "    X.extend(X2)\n",
    "    return classifier(X)[0]\n",
    "\n",
    "def text_prob(text):\n",
    "    text= get_cleaned_text(text)\n",
    "    X = [1] #Interceptor\n",
    "    X2 = count_vector.fit_transform([text]).toarray()[0]\n",
    "    X.extend(X2)\n",
    "    return probability(X)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vectorisation\n",
    "predictors  = pd.read_csv(\"./predictors.csv\")\n",
    "vocabulary = word_tokenize(' '.join(predictors))\n",
    "count_vector = CountVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "def get_vector(text):\n",
    "    array_vector = count_vector.fit_transform([text]).toarray()[0]\n",
    "    return array_vector\n",
    "\n",
    "def classifier(X):\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(1, -1)\n",
    "    return clf.predict(X)\n",
    "\n",
    "def probability(X):\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(1, -1)\n",
    "    return clf.predict_proba(X)\n",
    "\n",
    "def text_classify(text):\n",
    "    text= cleaned_text(text)\n",
    "    X = [1] #Interceptor\n",
    "    X2 = count_vector.fit_transform([text]).toarray()[0]\n",
    "    X.extend(X2)\n",
    "    return classifier(X)[0]\n",
    "\n",
    "def text_prob(text):\n",
    "    text= cleaned_text(text)\n",
    "    X = [1] #Interceptor\n",
    "    X2 = count_vector.fit_transform([text]).toarray()[0]\n",
    "    X.extend(X2)\n",
    "    return probability(X)[0][1]\n",
    "\n",
    "def create_vector_file(df, path):\n",
    "    # Clean text on my Dataframe\n",
    "    train = clean_text(df)\n",
    "    \n",
    "def vector_prediction(index,vector):\n",
    "    global df\n",
    "    v= vector[0:116].values.tolist()\n",
    "    X = [1] #Interceptor\n",
    "    X.extend(v)\n",
    "    prediction = classifier(X)[0]\n",
    "    prob = probability(X)[0][1]\n",
    "    df.set_value(index, columns,[prediction, prob, vector['ID'], vector['IDUSER'], vector['LOCATION'], vector['LAT'], vector['LON'], vector['TIMEZONE']],takeable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data09' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e516a1efc0a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_vector_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata09\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./data_vectorised/data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweetDB-AU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#add_geodata_vector_file('./data_vectorised/data/tweetDB-AU-29-Nov.csv','./data_vectorised/data/geodata_tweetDB-AU-29-Nov.csv', data09)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data09' is not defined"
     ]
    }
   ],
   "source": [
    "create_vector_file(data09,'./data_vectorised/data','tweetDB-AU')\n",
    "add_geodata_vector_file('./data_vectorised/data/tweetDB-AU-29-Nov.csv','./data_vectorised/data/geodata_tweetDB-AU-29-Nov.csv', data09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geodata  = pd.read_csv(\"./data_vectorised/data/geodata_tweetDB-AU-01-Nov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in geodata.iterrows():\n",
    "    vector_prediction(index,row)\n",
    "\n",
    "df.to_csv('./data_vectorised/data/results_tweetDB-AU-01-Nov.csv',sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.loc[df['PROBABILITY'] > 0.2]\n",
    "geodata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
