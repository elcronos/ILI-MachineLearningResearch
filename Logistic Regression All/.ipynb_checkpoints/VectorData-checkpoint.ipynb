{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import numpy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from pandas import Series\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RegexpStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classifier Load\n",
    "clf = joblib.load('./model/modelLogistic.pkl')\n",
    "\n",
    "def readjson(path):\n",
    "    return pd.read_json(path)\n",
    "\n",
    "def loadjson(path):\n",
    "    # read the entire file into a python array\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.readlines()\n",
    "    # remove the trailing \"\\n\" from each line\n",
    "    #with open('./random_sample.json', 'rb') as f:\n",
    "    #    data = f.readlines()\n",
    "    data = map(lambda x: x.rstrip(), data)\n",
    "    data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "    print data_json_str\n",
    "    # now, load it into pandas\n",
    "    sample = pd.read_json(data_json_str)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TweetsAU\n",
    "data09 = loadjson(\"./data/tweetDB-AU.json\")\n",
    "'''\n",
    "data01 = loadjson(\"./data/tweetDB-AU-15-Oct.json\")\n",
    "data02 = loadjson(\"./data/tweetDB-AU-05-Oct.json\")\n",
    "data03 = loadjson(\"./data/tweetDB-AU-15-Oct.json\")\n",
    "data04 = loadjson(\"./data/tweetDB-AU-19-Oct.json\")\n",
    "data05 = loadjson(\"./data/tweetDB-AU-25-Oct.json\")\n",
    "data06 = loadjson(\"./data/tweetDB-AU-30-Oct.json\")\n",
    "data07 = loadjson(\"./data/tweetDB-AU-01-Nov.json\")\n",
    "data08 = loadjson(\"./data/tweetDB-AU-Nov.json\")\n",
    "data09 = loadjson(\"./data/tweetDB-AU-Nov-25.json\")\n",
    "data10 = loadjson(\"./data/tweetDB-AU-Nov-29.json\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data09' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bdec33a433d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata09\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data09' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Word Stops\n",
    "stopset = set(stopwords.words('english'))\n",
    "morewords = [\"'s\", \"swine\", \"bird\", \"h1n1\", \"'ve\", \"lol\", \"pig\"]\n",
    "stopset.update(morewords)\n",
    "#Remove word from stopword list\n",
    "itemsToRemove = ['can','am', 'are', 're', 'm','have','has','i', 'you', 'he', 'she', 'we', 'they']\n",
    "stopset = [x for x in stopset if x not in itemsToRemove]\n",
    "\n",
    "#Vectorisation\n",
    "predictors  = pd.read_csv(\"./predictors.csv\")\n",
    "vocabulary = word_tokenize(' '.join(predictors))\n",
    "count_vector = CountVectorizer(vocabulary=vocabulary)\n",
    "#Predictors String\n",
    "predictor_list=list(predictors)\n",
    "predictors_str = ','.join(predictor_list)\n",
    "\n",
    "#Methods\n",
    "# Remove URLs, RTs, and twitter handles\n",
    "def clean_data(text):\n",
    "    #text= text.decode('utf-8')\n",
    "    text = text.replace('[^\\x00-\\x7F]','')\n",
    "    words = [text for text in text.split() if 'http' not in text and not text.startswith('@') and text != 'RT']\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Text to Lower Case\n",
    "def text_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove some characters\n",
    "def remove_special_characters(text):\n",
    "    bad_chars = '-#?(){}<>:;.!$%&/=+*^-`\\'0123456789'\n",
    "    rgx = re.compile('[%s]' % bad_chars)\n",
    "    return rgx.sub('', text)\n",
    "\n",
    "# Create a set of Stopwords\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopset]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopset:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Stemming words\n",
    "def stem_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    #Regex for Suffixes\n",
    "    st = RegexpStemmer('ing$|s$|able$|ible$|ful$|less$|ive$|acy$|al$|ance$|ence$|dom$|er$|or$|ism$|ist$|ity$|ty$|ment$|ship$|sion$|tion$|ate$|en$|ify$|fy$|ize$|ise$', min=4)\n",
    "    stemmed = []\n",
    "    for word in words:\n",
    "        stemmed.append(st.stem(word))\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def get_cleaned_text(text):\n",
    "    try:\n",
    "        cleaned_text= clean_data(text)\n",
    "        cleaned_text= text_to_lower(cleaned_text)\n",
    "        cleaned_text= remove_special_characters(cleaned_text)\n",
    "        cleaned_text= remove_stopwords(cleaned_text)\n",
    "        cleaned_text= stem_words(cleaned_text)\n",
    "    except Exception: \n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def clean_text(df):\n",
    "    for i, row in df.iterrows():\n",
    "        cleaned_text = row['text']\n",
    "        cleaned_text = get_cleaned_text(cleaned_text)\n",
    "        df.set_value(i,'text',cleaned_text)\n",
    "    return df\n",
    "\n",
    "def get_vector(text):\n",
    "    array_vector = count_vector.fit_transform([text]).toarray()[0]\n",
    "    return array_vector\n",
    "\n",
    "def classifier(X):\n",
    "    return clf.predict(X)\n",
    "\n",
    "def probability(X):\n",
    "    return clf.predict_proba(X)\n",
    "\n",
    "def text_classify(text):\n",
    "    text= get_cleaned_text(text)\n",
    "    X = [1] #Interceptor\n",
    "    X2 = count_vector.fit_transform([text]).toarray()[0]\n",
    "    X.extend(X2)\n",
    "    return classifier(X)[0]\n",
    "\n",
    "def text_prob(text):\n",
    "    text= get_cleaned_text(text)\n",
    "    X = [1] #Interceptor\n",
    "    X2 = count_vector.fit_transform([text]).toarray()[0]\n",
    "    X.extend(X2)\n",
    "    return probability(X)[0][1]\n",
    "\n",
    "def create_vector_file(df, path,name):\n",
    "    data = clean_text(df)\n",
    "    list_df = data['text'].tolist()\n",
    "    array_df = count_vector.fit_transform(list_df).toarray()\n",
    "    numpy.savetxt(path+'/'+name+'.csv', numpy.asarray(array_df), fmt='%i', delimiter=\",\", header=predictors_str)\n",
    "\n",
    "def add_geodata_vector_file(path, output, df):\n",
    "    foo =  pd.read_csv(path)\n",
    "    foo['ID'] = Series(df['id_tweet'], index=foo.index)\n",
    "    foo['IDUSER'] = Series(df['id_user'], index=foo.index)\n",
    "    foo['DATE'] = Series(df['created_at'], index=foo.index)\n",
    "    foo['LOCATION'] = Series(df['location'], index=foo.index)\n",
    "    foo['LAT'] = Series(df['lat'], index=foo.index)\n",
    "    foo['LON'] = Series(df['lon'], index=foo.index)\n",
    "    foo['TIMEZONE'] = Series(df['created_at'], index=foo.index)\n",
    "    foo.to_csv(output,sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create_vector_file(data09,'./data_vectorised/data','tweetDB-AU-30-Nov')\n",
    "#add_geodata_vector_file('./data_vectorised/data/tweetDB-AU-29-Nov.csv','./data_vectorised/data/geodata_tweetDB-AU-29-Nov.csv', data09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictors_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.360437370236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "text= 'This Flu can *** right off!'\n",
    "print (int)(text_classify(text)), text_prob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
