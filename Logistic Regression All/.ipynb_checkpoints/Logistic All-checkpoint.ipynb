{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from scikitplot import plotters as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import RegexpStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "\n",
    "train3 = pd.read_json(\"../../data/RelatedVsNotRelated.json\")\n",
    "train2 = pd.read_json(\"../../data/AwarenessVsInfection.json\")\n",
    "train = pd.read_json(\"../../data/SelfVsOthers.json\")\n",
    "\n",
    "# ## Related Vs Not Related:\n",
    "#  0: Not related to influenza\n",
    "#  1: Related to influenza\n",
    "train_notrelated = train3.loc[train3['type'] == 0]\n",
    "\n",
    "# ## Awareness Vs Infection\n",
    "# 0: Influenza infection\n",
    "# 1: Influenza awareness\n",
    "train_awareness = train2.loc[train2['type'] == 1]\n",
    "\n",
    "# ## Self Vs Others\n",
    "# 0: Others (the tweet describes someone else)\n",
    "# 1: Self (the tweet describes the author)\n",
    "train_others = train.loc[train['type'] == 0]\n",
    "train_self = train.loc[train['type'] == 1]\n",
    "\n",
    "# Define Word Stops\n",
    "stopset = set(stopwords.words('english'))\n",
    "morewords = [\"'s\", \"swine\", \"bird\", \"h1n1\", \"'ve\", \"lol\", \"pig\"]\n",
    "stopset.update(morewords)\n",
    "#Remove word from stopword list\n",
    "itemsToRemove = ['can','am', 'are', 're', 'm','have','has','i', 'you', 'he', 'she', 'we', 'they']\n",
    "stopset = [x for x in stopset if x not in itemsToRemove]\n",
    "\n",
    "#Methods\n",
    "# Remove URLs, RTs, and twitter handles\n",
    "def clean_data(text):\n",
    "    text= text.decode('utf-8')\n",
    "    text = text.replace('[^\\x00-\\x7F]','')\n",
    "    words = [text for text in text.split() if 'http' not in text and not text.startswith('@') and text != 'RT']\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Text to Lower Case\n",
    "def text_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove some characters\n",
    "def remove_special_characters(text):\n",
    "    bad_chars = '-#?(){}<>:;.!$%&/=+*^-`\\'0123456789'\n",
    "    rgx = re.compile('[%s]' % bad_chars)\n",
    "    return rgx.sub('', text)\n",
    "\n",
    "# Create a set of Stopwords\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopset]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopset:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Stemming words\n",
    "def stem_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    #Regex for Suffixes\n",
    "    st = RegexpStemmer('ing$|s$|able$|ible$|ful$|less$|ive$|acy$|al$|ance$|ence$|dom$|er$|or$|ism$|ist$|ity$|ty$|ment$|ship$|sion$|tion$|ate$|en$|ify$|fy$|ize$|ise$', min=4)\n",
    "    stemmed = []\n",
    "    for word in words:\n",
    "        stemmed.append(st.stem(word))\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    for i, row in df.iterrows():\n",
    "      cleaned_text = row['text']\n",
    "      cleaned_text= clean_data(cleaned_text)\n",
    "      cleaned_text= text_to_lower(cleaned_text)\n",
    "      cleaned_text= remove_special_characters(cleaned_text)\n",
    "      cleaned_text= remove_stopwords(cleaned_text)\n",
    "      cleaned_text= stem_words(cleaned_text)\n",
    "      df.set_value(i,'text',cleaned_text)\n",
    "    return df\n",
    "\n",
    "def create_wordcloud(list_words, name_cloud):\n",
    "    wordcloud = WordCloud(\n",
    "                      stopwords= stopset,\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(list_words)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('./wordclouds/'+name_cloud, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def print_frequency(words, number):\n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    for word, frequency in fdist.most_common(number):\n",
    "        print('{},'.format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series key provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cd13661c4eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_awareness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_awareness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_infection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_awareness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_notrelated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_awareness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_self\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_tuple\u001b[0;34m(self, key, is_setter)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(ax, key)\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unalignable boolean Series key provided'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1817\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexingError\u001b[0m: Unalignable boolean Series key provided"
     ]
    }
   ],
   "source": [
    "# Concat three training datasets\n",
    "# Change Awareness value 1 to 0\n",
    "\n",
    "train_awareness.loc[train_awareness.type == 1, 'type'] = 0\n",
    "frames = [train_notrelated, train_awareness, train_self]\n",
    "result = pd.concat(frames)\n",
    "#Drop Duplicate\n",
    "result = result.drop_duplicates(subset=['text'], keep=False)\n",
    "# Unique ID\n",
    "#len(result.text.unique())\n",
    "header= ['id', 'text', 'type']\n",
    "#result.to_csv('train.csv',index=False, sep=',',columns= header,encoding='utf-8')\n",
    "train_all= pd.read_csv(\"./trainall.csv\")\n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clean text on my Dataframe\n",
    "train_all = clean_text(train_all)\n",
    "#Vectorisation\n",
    "most_common  = pd.read_csv(\"./predictors.csv\")\n",
    "vocabulary = word_tokenize(' '.join(most_common))\n",
    "\n",
    "cv_all = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocabulary)\n",
    "list_all = train_all['text'].tolist()\n",
    "\n",
    "array_all = cv_all.fit_transform(list_all).toarray()\n",
    "# Create CSV file\n",
    "#np.savetxt(\"./data_vectorised/all.csv\", np.asarray(array_all.astype(int)), fmt='%i', delimiter=\",\")\n",
    "#foo =  pd.read_csv(\"./data_vectorised/all.csv\")\n",
    "#foo['RESULT'] = Series(train_all['type'], index=foo.index)\n",
    "#foo['ID'] = Series(train_all['id'], index=foo.index)\n",
    "#foo.to_csv('./data_vectorised/data.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812060301508\n",
      "0.829519656851\n",
      "[[715  36]\n",
      " [151  93]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.95      0.88       751\n",
      "        1.0       0.72      0.38      0.50       244\n",
      "\n",
      "avg / total       0.80      0.81      0.79       995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Logistic Regression\n",
    "data = pd.read_csv(\"./data_vectorised/data.csv\")\n",
    "y, X = dmatrices(\"RESULT ~ flu + gett + i + im  + he + school + sleep + dr + catch + make + tomorrow + since + damn + bit + great + keep + h + tired + first + soon + everyone + away + head + thought + someth + ready + next + start + com + fuck + may + little + anyone + lot + body + doct + could + long + god + seem + night + man + care + ok + done + look + stay + weekend + say + eith + nose + isnt + tonight + tell + office + regular + hand + shit + enough + come + rest + mask + help + please + fun + stomach + would + re + yes + sure + stupid + viru + nervou + due + crap + tak + cause + l + hopefully + life + old + wond + yeah + hell + health + woke + disease + clinic + every + must + suck + many + someone + actually + least + headache + kind + nas + concerned + havent + nev + hurt + youre + love + girl + friend + read + hour + hard + b + also + bc + us + anyth + g + ch\", data, return_type = 'dataframe')\n",
    "# flatten y into a 1-D array\n",
    "y = np.ravel(y)\n",
    "\n",
    "# instantiate a logistic regression model, and fit with X and y\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X,y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(X, y)\n",
    "\n",
    "# Model Evaluation Using a Validation Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "#print predicted\n",
    "y_score = model.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "\n",
    "# generate class probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "#print probs\n",
    "\n",
    "# generate evaluation metrics\n",
    "print metrics.accuracy_score(y_test, predicted)\n",
    "print metrics.roc_auc_score(y_test, probs[:, 1])\n",
    "\n",
    "\n",
    "print metrics.confusion_matrix(y_test, predicted)\n",
    "print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "probas = model.predict_proba(X)\n",
    "skplt.plot_roc_curve(y_true=y, y_probas=probas)\n",
    "plt.show()\n",
    "# Confusion Matrix\n",
    "preds = model.predict(X)\n",
    "skplt.plot_confusion_matrix(y_true=y, y_pred=preds)\n",
    "plt.show()\n",
    "#Learning Curve\n",
    "#skplt.plot_learning_curve(model, X, y)\n",
    "#plt.show()\n",
    "\n",
    "#Precision Recall Curve\n",
    "skplt.plot_precision_recall_curve(y_true=y, y_probas=probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
