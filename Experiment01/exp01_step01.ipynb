{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "train = pd.read_json(\"../../data/RelatedVsNotRelated.json\")\n",
    "train2 = pd.read_json(\"../../data/AwarenessVsInfection.json\")\n",
    "train3 = pd.read_json(\"../../data/SelfVsOthers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Vs Not Related:\n",
    " 0: Not related to influenza\n",
    " 1: Related to influenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_not_related = train.loc[train['type'] == 0]\n",
    "train_related = train.loc[train['type'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awareness Vs Infection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: Influenza infection\n",
    "1: Influenza awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_infection = train2.loc[train2['type'] == 0]\n",
    "train_awareness = train2.loc[train2['type'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Vs Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: Others (the tweet describes someone else)\n",
    "1: Self (the tweet describes the author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_others = train3.loc[train3['type'] == 0]\n",
    "train_self = train3.loc[train3['type'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Word Stops\n",
    "stopset = set(stopwords.words('english'))\n",
    "morewords = ['who','which','isn\\'t','aren\\'t', 'I\\'m','\\'m']\n",
    "stopset.update(morewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove URLs, RTs, and twitter handles\n",
    "def clean_data(text):\n",
    "    text = text.replace('[^\\x00-\\x7F]','')\n",
    "    words = [text for text in text.split() if 'http' not in text and not text.startswith('@') and text != 'RT']\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Text to Lower Case\n",
    "def text_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove some characters\n",
    "def remove_special_characters(text):\n",
    "    bad_chars = '#?(){}<>:;.!$%&/=+*^-'\n",
    "    \n",
    "    rgx = re.compile('[%s]' % bad_chars)\n",
    "    return rgx.sub('', text)\n",
    "\n",
    "# Create a set of Stopwords\n",
    "def remove_stopwords(text):  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopset]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopset:\n",
    "            filtered_sentence.append(w)\n",
    "        \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Stemming words\n",
    "def stem_words(text):\n",
    "    #Regex for Suffixes\n",
    "    st = RegexpStemmer('ing$|s$|e$|able$|ible$|ful$|less$|ive$|acy$|al$|ance$|ence$|dom$|er$|or$|ism$|ist$|ity$|ty$|ment$|ship$|sion$|tion$|ate$|en$|ify$|fy$|ize$|ise$', min=4)\n",
    "    \n",
    "    stemmed = []\n",
    "    \n",
    "    for word in words:\n",
    "        stemmed.append(st.stem(word))\n",
    "        \n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    for i, row in df.iterrows():\n",
    "      cleaned_text = row['text']\n",
    "      cleaned_text= clean_data(cleaned_text)\n",
    "      cleaned_text= text_to_lower(cleaned_text)\n",
    "      cleaned_text= remove_special_characters(cleaned_text)\n",
    "      cleaned_text= remove_stopwords(cleaned_text)\n",
    "      #cleaned_text= stem_words(cleaned_text)\n",
    "      df.set_value(i,'text',cleaned_text)\n",
    "    return df\n",
    "\n",
    "def create_wordcloud(list_words, name_cloud):\n",
    "    wordcloud = WordCloud(\n",
    "                      stopwords= stopset,\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(list_words)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('./wordclouds/'+name_cloud, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "def print_frequency(words, number):\n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    for word, frequency in fdist.most_common(number):\n",
    "        print('{}: {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean text on my Dataframe\n",
    "train_related = clean_text(train_related)\n",
    "train_not_related= clean_text(train_not_related)\n",
    "\n",
    "train_infection = clean_text(train_infection)\n",
    "train_awareness = clean_text(train_awareness)\n",
    "\n",
    "train_others = clean_text(train_others)\n",
    "train_self = clean_text(train_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Wordcloud\n",
    "list1 = ' '.join(train_related['text'])\n",
    "list2 = ' '.join(train_not_related['text'])\n",
    "list3 = ' '.join(train_infection['text'])\n",
    "list4 = ' '.join(train_awareness['text'])\n",
    "list5 = ' '.join(train_others['text'])\n",
    "list6 = ' '.join(train_self['text'])\n",
    "\n",
    "#create_wordcloud(list1, 'wordcloud_related')\n",
    "#create_wordcloud(list2, 'wordcloud_not_related')\n",
    "\n",
    "#create_wordcloud(list3, 'wordcloud_infection')\n",
    "#create_wordcloud(list4, 'wordcloud_awareness')\n",
    "\n",
    "#create_wordcloud(list5, 'wordcloud_others')\n",
    "#create_wordcloud(list6, 'wordcloud_self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flu: 2951\n",
      "getting: 1546\n",
      "swine: 1143\n",
      "'s: 428\n",
      "shot: 370\n",
      "n't: 352\n",
      "bird: 300\n",
      "sick: 292\n",
      "get: 256\n",
      "worried: 243\n",
      "h1n1: 178\n",
      "think: 173\n",
      "like: 172\n",
      "got: 172\n",
      "scared: 164\n",
      "better: 141\n",
      "thinking: 138\n",
      "hope: 136\n",
      "worry: 128\n",
      "im: 128\n",
      "fear: 122\n",
      "vaccine: 115\n",
      "today: 109\n",
      "going: 104\n",
      "feel: 103\n",
      "one: 102\n",
      "still: 95\n",
      "u: 95\n",
      "people: 94\n",
      "good: 94\n",
      "shots: 90\n",
      "really: 89\n",
      "``: 86\n",
      "home: 85\n",
      "afraid: 84\n",
      "lol: 83\n",
      "need: 82\n",
      "everyone: 82\n",
      "cold: 81\n",
      "feeling: 80\n",
      "'': 78\n",
      "go: 78\n",
      "know: 74\n",
      "2: 70\n",
      "time: 69\n",
      "week: 68\n",
      "oh: 66\n",
      "might: 65\n",
      "work: 63\n",
      "day: 62\n"
     ]
    }
   ],
   "source": [
    "#Tokenize DF\n",
    "list1 = nltk.tokenize.word_tokenize(' '.join(train_related['text']))\n",
    "# Output top 50 words\n",
    "print_frequency(list1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
